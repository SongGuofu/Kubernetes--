# 存储卷与数据持久化

版权声明：该文档由[马哥教育](http://www.magedu.com)原创，未经书面允许，严禁转载，严禁一切形式的商业用途。

## 实验介绍

Pod需要借助于存储卷完成独立于自身生命周期的数据持久化存储，以及在同一Pod内部的多个容器间进行数据交换和共享。在本实验场景中，我们会带你学习使用Pod支持的多种类型的存储卷，这包括：

- emptyDir存储卷
- hostPath存储卷
- nfs存储卷
- 基于nfs-csi-driver和nfs的PV、PVC和StorageClass

在Pod中定义使用存储卷的配置由两部分组成：一是通过.spec.volumes字段定义在Pod之上的存储卷列表，它经由特定的存储卷插件并结合特定的存储供给方的访问接口进行定义；另一个是嵌套定义在容器的volumeMounts字段上的存储卷挂载列表，它只能挂载当前Pod对象中定义的存储卷。

## Step2: emptyDir存储卷

emptyDir存储卷可以理解为Pod对象上的一个临时目录，在Pod对象启动时自动创建，而在Pod对象被移除时一并被删除。因此，emptyDir存储卷只能用于某些特殊场景中，例如同一Pod内的多个容器间文件共享，或作为容器数据的临时存储目录用于数据缓存系统等。

下面是一个使用了emptyDir存储卷的简单示例，将内容存储于文件中，例如volumes-emptydir-demo.yaml，即可进行后面的测试。

```yaml
# Created-By:"MageEdu <mage@magedu.com>"
apiVersion: v1
kind: Pod
metadata:
  name: volumes-emptydir-demo
spec:
  initContainers:
  - name: config-file-downloader
    image: ikubernetes/admin-box
    imagePullPolicy: IfNotPresent
    command: ['/bin/sh','-c','wget -O /data/envoy.yaml https://code.aliyun.com/MageEdu/kubernetes-start/raw/master/tutorials/handson/volume-basics-01/envoy.yaml']
    volumeMounts:
    - name: config-file-store
      mountPath: /data
      containers:
  - name: envoy
    image: envoyproxy/envoy-alpine:v1.14.1
    command: ['/bin/sh','-c']
    args: ['envoy -c /etc/envoy/envoy.yaml']
    volumeMounts:
    - name: config-file-store
      mountPath: /etc/envoy
      readOnly: true
      volumes:
  - name: config-file-store
    emptyDir:
      medium: Memory
      sizeLimit: 16Mi
```

运行如下命令，创建Pod对象liveness-exec-demo。

```bash
kubectl apply -f volumes-emptydir-demo.yaml
```


在Pod的详细描述信息中，会有显示存储卷的状态及容器上对该存储卷的挂载状态。

```bash
kubectl describe pods volumes-emptydir-demo
```

该命令会输出类似如下内容。

```console
……
Init Containers:
  config-file-downloader:
  ……
    Mounts:
      /data from config-file-store (rw)
  ……
Containers:
  envoy:
    Mounts:
      /etc/envoy from config-file-store (ro)
  ……
Volumes:
  config-file-store:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  16Mi
……
```

为Envoy下载的配置文件中定义了一个监听于所有可用IP地址上80/TCP端口的Ingress侦听器，以及一个监听于所有可用IP地址上9901/TCP端口的Admin接口，这与Envoy镜像中默认配置文件中的定义均有不同。

```bash
kubectl exec volumes-emptydir-demo -- netstat -tnl
```

类似于下面命令的结果显示它吻合自定义配置文件的内容。

```console
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      
tcp        0      0 0.0.0.0:9901            0.0.0.0:*               LISTEN  
```


我们也可以向该服务的9901端口发起请求，查看envoy内部是否配置有我们自定义的集群。首先，使用如下命令获取pods/volumes-emptydir-demo的IP地址，并启动一个测试专用的交互式Pod，将该IP地址以环境变量形式注入到容器中，以便于向pods/volumes-emptydir-demo发起服务访问请求。

```bash
podIP=$(kubectl get pods/volumes-emptydir-demo -o jsonpath={.status.podIP})
kubectl run pod-$RANDOM --image="ikubernetes/admin-box:v1.0" --restart=Never --env=podIP=$podIP --rm -it --command -- curl -s http://$podIP:9901/
```

而后，在打开的交互式接口中，测试对Pod中服务的访问请求。
```bash
curl -s $podIP:9901/clusters
```

其输出结果应该类似如下所示。

```console
mageedu::default_priority::max_connections::1024
mageedu::default_priority::max_pending_requests::1024
mageedu::default_priority::max_requests::1024
mageedu::default_priority::max_retries::3
mageedu::high_priority::max_connections::1024
mageedu::high_priority::max_pending_requests::1024
mageedu::high_priority::max_requests::1024
mageedu::high_priority::max_retries::3
mageedu::added_via_api::false
mageedu::127.0.0.1:8080::cx_active::0
mageedu::127.0.0.1:8080::cx_connect_fail::0
...
```

## Step3：hostPath存储卷

hostPath卷插件是用于将工作节点上某文件系统的目录或文件关联到Pod上的一种存储卷类型，其数据具有同工作节点生命周期的持久性。换句话说，若Pod两次被调度至非同一节点，则前一节点上生成的保存于存储卷中的数据，将无法在当前节点上的Pod中访问。

下面的示例中，容器中的filebeat进程负责收集工作节点及容器相关的日志信息发往redis服务器，它使用了3个hostPath类型的存储卷，第一个指向了宿主机的日志文件目录/var/logs，后面两个则跟宿主机上的Docker运行时环境有关。将清单中的资源规范保存于文件中，例如volumes-hostpath-demo.yaml，即可进行测试。

```yaml
# Created-By:"MageEdu <mage@magedu.com>"
apiVersion: v1
kind: Pod
metadata:
  name: volumes-hostpath-demo
  labels:
    app: redis
spec:
  containers:
  - name: redis
    image: redis:alpine
    ports:
    - containerPort: 6379
      name: redisport
      securityContext:
      runAsUser: 999
      volumeMounts:
    - mountPath: /data
      name: redisdata
      volumes:
    - name: redisdata
      hostPath:
        path: /appdatas/redis/
        type: DirectoryOrCreate
```

运行如下命令，创建Pod对象liveness-exec-demo。

```bash
kubectl apply -f volumes-hostpath-demo.yaml
```

等Pod资源创建完成后，我们可以先了解一下其被调度到的节点。

```bash
kubectl get pods/volumes-hostpath-demo -o jsonpath={.status.hostIP}
```

```console
192.168.0.28
```

随后，等Pod进入Running状态后，可通过其命令客户端redis-cli创建测试数据，并手动触发其同步于存储系统中。

```bash
kubectl exec -it volumes-hostpath-demo -- redis-cli
```

以下内容中，“127.0.0.1:6379>”为redis容器的交互式接口，其后是需要在接口中键入的测试命令。

```console
127.0.0.1:6379> set mykey "Hi MageEdu"
OK
127.0.0.1:6379> get mykey
"Hi MageEdu"
127.0.0.1:6379> BGSAVE
Background saving started
127.0.0.1:6379> exit
```

为了测试其数据持久化效果，我们可删除此前创建Pod对象volumes-hostpath-demo，而后待重建该Pod对象后检测数据是否依然能够访问。

```bash
kubectl delete -f volumes-hostpath-demo.yaml && kubectl apply -f volumes-hostpath-demo.yaml 
```

资源创建完成后，我们可以先了解一下其被调度到的节点。

```bash
kubectl get pods/volumes-hostpath-demo -o jsonpath={.status.hostIP}
```

```console
192.168.0.28
```

待其重建完成后，通过再一次创建的Pod资源的详细描述信息可以观察到它挂载使用NFS存储卷的相关状态，也可通过下面的命令来检查redis-server中是否还保存有此前存储的数据。
```bash
kubectl exec -it volumes-hostpath-demo -- redis-cli
```

事实上，若新的Pod对象能够被调度器调度至此前绑定的节点之上，其数据则可以再次获取到，否则，其内容其无从得到。以下内容中，“127.0.0.1:6379>”为redis容器的交互式接口，其后是需要在接口中键入的测试命令。

```console
127.0.0.1:6379> get mykey
"Hi MageEdu"
127.0.0.1:6379> exit
```

需要注意是，若Pod第二次绑定的节点与第一次有所不同，则第二次测试将无法获取到相应的键值。此时，就需要具有跨节点共享数据功能的网络存储卷了。

## Step4：nfs存储卷

NFS存储卷插件支持在Pod上，借助其所属的节点驱动并挂载NFS上导出（export）的目录给Pod内部的容器使用。我们首先得准备一台主机作为NFS Server以便于测试。

假设这台主机为nfs.magedu.com，而Kubernetes集群的各节点也已经解析该名称到了正确的IP地址上，且该主机运行于Ubuntu Server 2004。我们首先要在该主机上运行如下命令安装NFS Server。

```bash
~# apt install nfs-kernel-server
```

随后，在该主机上导出必要的目录，作为可用存储服务路径。以/data/redis为例，我们在nfs.magedu.com主机上编辑/etc/exports文件，添加类似如下内容，其中的“172.29.0.0/16”是Kubernetes集群各节点所在的网络。

```
/data/redis    172.29.0.0/16(rw,no_subtree_check)
```

再运行如下命令，导出新增的目录给客户端使用。

```bash
~# exportfs -avr
```

接下来，我们需要在在Kubernetes集群的各节点上运行如下命令，安装必要的nfs客户端组件。

```bash
~# apt install nfs-common
```

而后创建如下Pod，提交给API Server，即可使用nfs卷。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-nfs-vol
  labels:
    app: redis
spec:
  containers:
  - name: redis
    image: redis:alpine
    ports:
    - containerPort: 6379
      name: redisport
    volumeMounts:
    - mountPath: /data
      name: redisdata
  volumes:
    - name: redisdata
      nfs:
        server: nfs.magedu.com
        path: /data/redis
        readOnly: false
```

随后，等Pod进入Running状态后，可通过其命令客户端redis-cli创建测试数据来测试网络存储卷的数据持久性功能。具体步骤与前一节相同，但用户再也不用关心重建后的Pod是否会被调度至其它节点。

## Step5：PV、PVC和StorageClass

请参考[csi-driver-nfs](https://github.com/iKubernetes/learning-k8s/tree/master/csi-driver-nfs)一节中的中的内容。


## Step6：清理创建的所有Pod对象

执行如下命令，查看创建的Pod资源。

```bash
kubectl get pods
```

确认测试完成后，即可删除所有的Pod对象。

```bash
kubectl delete pods --all --force --grace-period=0
```

- 恭喜你完成本教程，欢迎继续挑战后面的其它实验场景~~
- 关于存储卷的更详细介绍，请参阅《Kubernetes进阶实战(第2版)》第5章。

### 参考资料：
1. 《Kubernetes进阶实战(第2版)》
2. 《Kubernetes进阶实战(第2版)》随书[源码](https://github.com/iKubernetes)
3. Kubernetes项目官方文档
4. 搜索引擎搜索到的其它资料
